{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f2db10-4404-48c4-ba39-a274daacf149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처 : https://huggingface.co/google/paligemma-3b-pt-448\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_id = \"google/paligemma-3b-mix-448\"\n",
    "device = \"cuda:0\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device,\n",
    "    revision=\"bfloat16\",\n",
    ").eval()\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "# 소요된 시간을 계산합니다.\n",
    "end_time = time.time()\n",
    "formatted_elapsed_time = \"{:.2f}\".format(end_time - start_time)\n",
    "print(f'*time:{formatted_elapsed_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b155362-cce3-4df3-82e3-daf40997d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 폴더에 저장 (예: ./model/paligemma-3b-mix-448 폴더에 저장)\n",
    "folder = \"./model/paligemma-3b-mix-448\"\n",
    "model.save_pretrained(folder, safe_serialization=False)\n",
    "processor.save_pretrained(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9530e803-fd61-47a6-8d31-2350f7966c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4772676aca324c099cd7139a94ce333d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*time:6.52\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 로컬 파일에서 모델과 프로세서 불러오기\n",
    "folder = \"./model/paligemma-3b-mix-448\"\n",
    "device = \"cuda:0\"\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "    folder,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device,\n",
    "    revision=\"bfloat16\",\n",
    ").eval()\n",
    "processor = AutoProcessor.from_pretrained(folder)\n",
    "\n",
    "# 소요된 시간을 계산합니다.\n",
    "end_time = time.time()\n",
    "formatted_elapsed_time = \"{:.2f}\".format(end_time - start_time)\n",
    "print(f'*time:{formatted_elapsed_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5f97161-776c-4582-b43a-350acf79e82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this image we can see buildings, trees, plants, water, hills and sky with clouds.\n",
      "*time:0.60\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "#image = Image.open(requests.get(url, stream=True).raw)\n",
    "image = Image.open(\"./data/trap1.jpg\") # 로컬이미지 불러옴\n",
    "\n",
    "# Instruct the model to create a caption in Spanish\n",
    "prompt = \"Describe the image\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 500, \n",
    "        \"temperature\": 0.0, # 0.0 이면 출력고정 \n",
    "        \"do_sample\": False, \n",
    "    } \n",
    "    # generation = model.generate(**model_inputs, max_new_tokens=500, do_sample=False)\n",
    "    generation = model.generate(**model_inputs, **generation_args)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)\n",
    "    \n",
    "# 소요된 시간을 계산합니다.\n",
    "end_time = time.time()\n",
    "formatted_elapsed_time = \"{:.2f}\".format(end_time - start_time)\n",
    "print(f'*time:{formatted_elapsed_time}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e0d4e6e-accb-4d7c-8275-1f7bd5f07347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgeneration_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerationConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogits_processor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msynced_gpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0massistant_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PreTrainedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BaseStreamer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateBeamDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateBeamEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Generates sequences of token ids for models with a language modeling head.\n",
       "\n",
       "<Tip warning={true}>\n",
       "\n",
       "Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
       "model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
       "parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
       "\n",
       "For an overview of generation strategies and code examples, check out the [following\n",
       "guide](../generation_strategies).\n",
       "\n",
       "</Tip>\n",
       "\n",
       "Parameters:\n",
       "    inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
       "        The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
       "        method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
       "        should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
       "        `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
       "    generation_config ([`~generation.GenerationConfig`], *optional*):\n",
       "        The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
       "        passed to generate matching the attributes of `generation_config` will override them. If\n",
       "        `generation_config` is not provided, the default will be used, which has the following loading\n",
       "        priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
       "        configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
       "        default values, whose documentation should be checked to parameterize generation.\n",
       "    logits_processor (`LogitsProcessorList`, *optional*):\n",
       "        Custom logits processors that complement the default logits processors built from arguments and\n",
       "        generation config. If a logit processor is passed that is already created with the arguments or a\n",
       "        generation config an error is thrown. This feature is intended for advanced users.\n",
       "    stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
       "        Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
       "        generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
       "        generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
       "        sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
       "        intended for advanced users.\n",
       "    prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
       "        If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
       "        provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
       "        `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
       "        on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
       "        for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
       "        Retrieval](https://arxiv.org/abs/2010.00904).\n",
       "    synced_gpus (`bool`, *optional*):\n",
       "        Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
       "        `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
       "        generating before other GPUs. Otherwise it'll be set to `False`.\n",
       "    assistant_model (`PreTrainedModel`, *optional*):\n",
       "        An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
       "        same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
       "        is much faster than running generation with the model you're calling generate from. As such, the\n",
       "        assistant model should be much smaller.\n",
       "    streamer (`BaseStreamer`, *optional*):\n",
       "        Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
       "        through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
       "    negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
       "        size. This is an experimental feature, subject to breaking API changes in future versions.\n",
       "    negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
       "        Attention_mask for `negative_prompt_ids`.\n",
       "    kwargs (`Dict[str, Any]`, *optional*):\n",
       "        Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
       "        forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
       "        specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
       "\n",
       "Return:\n",
       "    [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
       "    or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
       "\n",
       "        If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
       "        [`~utils.ModelOutput`] types are:\n",
       "\n",
       "            - [`~generation.GenerateDecoderOnlyOutput`],\n",
       "            - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
       "\n",
       "        If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
       "        [`~utils.ModelOutput`] types are:\n",
       "\n",
       "            - [`~generation.GenerateEncoderDecoderOutput`],\n",
       "            - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/generation/utils.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25756106-ee4d-4559-bf3e-bb157577913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time:0.27\n",
      "Translated(src=en, dest=ko, text=이 이미지에서 우리는 구름이있는 건물, 나무, 식물, 물, 언덕 및 하늘을 볼 수 있습니다., pronunciation=i imijieseo ulineun guleum-iissneun geonmul, namu, sigmul, mul, eondeog mich haneul-eul bol su issseubnida., extra_data=\"{'confiden...\")\n",
      "\n",
      "이 이미지에서 우리는 구름이있는 건물, 나무, 식물, 물, 언덕 및 하늘을 볼 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# 번역 => 영문을 한국어로 번역\n",
    "#!pip install googletrans==4.0.0-rc1\n",
    "from googletrans import Translator\n",
    "def translate_google(text:str, source_lang:str, target_lang:str):\n",
    "    translator = Translator()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    res = translator.translate(text, src=source_lang, dest=target_lang)\n",
    "\n",
    "    # 소요된 시간을 계산합니다.\n",
    "    end_time = time.time()\n",
    "    formatted_elapsed_time = \"{:.2f}\".format(end_time - start_time)\n",
    "    print(f'time:{formatted_elapsed_time}')\n",
    "\n",
    "    print(res)\n",
    "    print()\n",
    "    response = res.text.strip('\"')\n",
    "    return response\n",
    "\n",
    "text = translate_google(text=decoded, source_lang='en', target_lang='ko')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a878c52f-5033-43ae-9421-c4c96cc89b88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
